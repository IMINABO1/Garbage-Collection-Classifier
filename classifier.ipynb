{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import keras.applications.xception as xception\n",
    "import zipfile\n",
    "import sys\n",
    "import time\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "import re\n",
    "\n",
    "from PIL import Image\n",
    "from keras.layers import Input, Conv2D, Dense, Flatten, MaxPooling2D, Input, GlobalAveragePooling2D\n",
    "from keras.layers.experimental.preprocessing import Normalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Lambda\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
    "import keras.applications.xception as xception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Giving constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 320    \n",
    "IMAGE_HEIGHT = 320\n",
    "IMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "IMAGE_CHANNELS = 3\n",
    "\n",
    "\n",
    "# Path where our data is located\n",
    "base_path = \"../data_Set/archive/garbage_classification/\"\n",
    "\n",
    "# Dictionary to save our 13 classes\n",
    "categories = {0: 'paper', 1: 'cardboard', 2: 'plastic', 3: 'metal', 4: 'trash', 5: 'battery',\n",
    "              6: 'shoes', 7: 'clothes', 8: 'green-glass', 9: 'brown-glass', 10: 'white-glass',\n",
    "              11: 'biological', 12:'electronics'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_test_path=\"../data_Set/archive/my_test/\"\n",
    "my_img=\"../data_Set/archive/my_test/clothes1.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Datframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add class name prefix to filename. So for example \"/paper104.jpg\" become \"paper/paper104.jpg\"\n",
    "def add_class_name_prefix(df, col_name):\n",
    "    df[col_name] = df[col_name].apply(lambda x: x[:re.search(\"\\d\",x).start()] + '/' + x)\n",
    "    return df\n",
    "\n",
    "# list conatining all the filenames in the dataset\n",
    "filenames_list = []\n",
    "# list to store the corresponding category, note that each folder of the dataset has one class of data\n",
    "categories_list = []\n",
    "\n",
    "for category in categories:\n",
    "    filenames = os.listdir(base_path + categories[category])\n",
    "    \n",
    "    filenames_list = filenames_list  +filenames\n",
    "    categories_list = categories_list + [category] * len(filenames)\n",
    "    \n",
    "df = pd.DataFrame({\n",
    "    'filename': filenames_list,\n",
    "    'category': categories_list\n",
    "})\n",
    "\n",
    "df = add_class_name_prefix(df, 'filename')\n",
    "\n",
    "# Shuffle the dataframe\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print('number of elements = ' , len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### view random sample img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see sample image, you can run the same cell again to get a different image\n",
    "random_row = random.randint(0, len(df)-1)\n",
    "sample = df.iloc[random_row]\n",
    "randomimage = Image.open(base_path +sample['filename'])\n",
    "print(sample['filename'])\n",
    "plt.imshow(randomimage)\n",
    "#randomimage.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See the categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visualization = df.copy()\n",
    "# Change the catgegories from numbers to names\n",
    "df_visualization['category'] = df_visualization['category'].apply(lambda x:categories[x] )\n",
    "\n",
    "df_visualization['category'].value_counts().plot.bar(x = 'count', y = 'category' )\n",
    "\n",
    "plt.xlabel(\"Garbage Classes\", labelpad=14)\n",
    "plt.ylabel(\"Images Count\", labelpad=14)\n",
    "plt.title(\"Count of images per class\", y=1.02);    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making the model\n",
    "\n",
    "#### Using Xception model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, Activation, BatchNormalization\n",
    "import keras.applications.xception as xception\n",
    "\n",
    "xception_layer = xception.Xception(include_top = False, input_shape = (IMAGE_WIDTH, IMAGE_HEIGHT,IMAGE_CHANNELS),\n",
    "                       weights = '../data_Set/Xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5/xception_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "\n",
    "# We don't want to train the imported weights\n",
    "xception_layer.trainable = False\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(keras.Input(shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS)))\n",
    "\n",
    "#create a custom layer to apply the preprocessing\n",
    "def xception_preprocessing(img):\n",
    "  return xception.preprocess_input(img)\n",
    "\n",
    "model.add(Lambda(xception_preprocessing))\n",
    "\n",
    "model.add(xception_layer)\n",
    "model.add(tf.keras.layers.GlobalAveragePooling2D())\n",
    "model.add(Dense(len(categories), activation='softmax')) \n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### this block is going to premarturely stop the training session if validation accuracy is not imporving for certain epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(patience = 2, verbose = 1, monitor='val_categorical_accuracy' , mode='max', min_delta=0.001, restore_best_weights = True)\n",
    "\n",
    "callbacks = [early_stop]\n",
    "\n",
    "print('call back defined!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset into training validation and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the categories from numbers to names\n",
    "df[\"category\"] = df[\"category\"].replace(categories) \n",
    "\n",
    "# We first split the data into two sets and then split the validate_df to two sets\n",
    "train_df, validate_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "validate_df, test_df = train_test_split(validate_df, test_size=0.5, random_state=42)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "validate_df = validate_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "total_train = train_df.shape[0]\n",
    "total_validate = validate_df.shape[0]\n",
    "\n",
    "print('train size = ', total_validate , 'validate size = ', total_validate, 'test size = ', test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create a generator for our training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "\n",
    "train_datagen = image.ImageDataGenerator(\n",
    "    \n",
    "    ###  Augmentation Start  ###\n",
    "    \n",
    "    #rotation_range=30,\n",
    "    #shear_range=0.1,\n",
    "    #zoom_range=0.3,\n",
    "    #horizontal_flip=True,\n",
    "    #vertical_flip = True,\n",
    "    #width_shift_range=0.2,\n",
    "    #height_shift_range=0.2\n",
    "    \n",
    "    ##  Augmentation End  ###\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df, \n",
    "    base_path, \n",
    "    x_col='filename',\n",
    "    y_col='category',\n",
    "    target_size=IMAGE_SIZE,\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_datagen = image.ImageDataGenerator()\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "    validate_df, \n",
    "    base_path, \n",
    "    x_col='filename',\n",
    "    y_col='category',\n",
    "    target_size=IMAGE_SIZE,\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "history = model.fit_generator(\n",
    "    train_generator, \n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=total_validate//batch_size,\n",
    "    steps_per_epoch=total_train//batch_size,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_another.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "ax1.plot(history.history['loss'], color='b', label=\"Training loss\")\n",
    "ax1.plot(history.history['val_loss'], color='r', label=\"validation loss\")\n",
    "ax1.set_yticks(np.arange(0, 0.7, 0.1))\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(history.history['categorical_accuracy'], color='b', label=\"Training accuracy\")\n",
    "ax2.plot(history.history['val_categorical_accuracy'], color='r',label=\"Validation accuracy\")\n",
    "ax2.legend()\n",
    "\n",
    "legend = plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = image.ImageDataGenerator()\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe= test_df,\n",
    "    directory=base_path,\n",
    "    x_col='filename',\n",
    "    y_col='category',\n",
    "    target_size=IMAGE_SIZE,\n",
    "    color_mode=\"rgb\",\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=1,\n",
    "    shuffle=False \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = test_generator.filenames\n",
    "nb_samples = len(filenames)\n",
    "\n",
    "_, accuracy = test_model.evaluate_generator(test_generator, nb_samples)\n",
    "\n",
    "print('accuracy on test set = ',  round((accuracy * 100),2 ), '% ') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rise",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
